{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mido\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mido\n",
    "\n",
    "def createMidiFileWithAllMessages(notes, duration=200, wait_time = 0, filename='test.mid'):\n",
    "    mid = mido.MidiFile()\n",
    "    track = mido.MidiTrack()\n",
    "    mid.tracks.append(track)\n",
    "\n",
    "    for note in notes:\n",
    "        track.append(mido.Message('note_on', note=note, velocity=64, time=wait_time))\n",
    "        track.append(mido.Message('note_off', note=note, velocity=0, time=duration))\n",
    "\n",
    "    mid.save(filename.removesuffix('.mid') + '.mid')\n",
    "\n",
    "\n",
    "notes = range(50, 80) \n",
    "\n",
    "createMidiFileWithAllMessages(notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "catch all the data\n",
      "['data\\\\chopin\\\\chpn-p1.mid', 'data\\\\chopin\\\\chpn-p10.mid', 'data\\\\chopin\\\\chpn-p11.mid', 'data\\\\chopin\\\\chpn-p12.mid', 'data\\\\chopin\\\\chpn-p13.mid', 'data\\\\chopin\\\\chpn-p14.mid', 'data\\\\chopin\\\\chpn-p15.mid', 'data\\\\chopin\\\\chpn-p16.mid', 'data\\\\chopin\\\\chpn-p17.mid', 'data\\\\chopin\\\\chpn-p18.mid']\n",
      "48\n"
     ]
    }
   ],
   "source": [
    "print('catch all the data')\n",
    "\n",
    "def get_midis_one_artist(artist):\n",
    "    ms = []\n",
    "    directory = \"data\"\n",
    "    directory = os.path.join(\"data\", artist)\n",
    "    for filename in os.listdir(directory):\n",
    "        if('V2'in filename): continue\n",
    "        if os.path.isfile(os.path.join(directory, filename)):\n",
    "            ms.append(os.path.join(directory, filename))\n",
    "            # print(filename)\n",
    "    return ms\n",
    "\n",
    "\n",
    "def get_midis():\n",
    "    ms = []\n",
    "    directory = \"data\"\n",
    "    for foldername in os.listdir(directory):\n",
    "        directory = os.path.join(\"data\", foldername)\n",
    "        for filename in os.listdir(directory):\n",
    "            if('V2'in filename): continue\n",
    "            if os.path.isfile(os.path.join(directory, filename)):\n",
    "                ms.append(os.path.join(directory, filename))\n",
    "                # print(filename)\n",
    "    return ms\n",
    "\n",
    "# midis = get_midis()\n",
    "midis = get_midis_one_artist('chopin')\n",
    "print(midis[:10])\n",
    "# works only with files with just one track\n",
    "def get_all_notes_one_file(midi_file):\n",
    "    file = mido.MidiFile(midi_file)\n",
    "    track = file.tracks[1]\n",
    "    messages = []\n",
    "    for message in track:\n",
    "        if(message.type == 'note_on'):\n",
    "            messages.append(message)\n",
    "    \n",
    "    notes = [0] + [m.note for m in messages] + [1]\n",
    "    # print(notes)\n",
    "    return notes\n",
    "\n",
    "all_notes_all_files = [get_all_notes_one_file(m) for m in midis]\n",
    "\n",
    "print(len(all_notes_all_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80706,)\n",
      "notes extreme to remove :  [  0   1  26  28  29  33  34  35  36  37  38  39  40  41  42  43  44  45\n",
      "  46  47 101]\n",
      "[12 12 19 16 16 12 12 19 21 21]\n"
     ]
    }
   ],
   "source": [
    "X_values = np.concatenate(all_notes_all_files)\n",
    "print(X_values.shape)\n",
    "\n",
    "# notes\n",
    "\n",
    "n = np.unique(X_values, return_counts=True)\n",
    "notes_extreme = n[0][n[1] < 100]\n",
    "print('notes extreme to remove : ', notes_extreme)\n",
    "X_values = X_values[~np.isin(X_values, notes_extreme)]\n",
    "\n",
    "nunique = np.unique(X_values, return_counts=True)\n",
    "\n",
    "# Tokenize the data\n",
    "\n",
    "tokenToVals = nunique[0]\n",
    "\n",
    "ValsToToken = {v:i for i, v in enumerate(tokenToVals)}\n",
    "vocab_size = len(tokenToVals)\n",
    "\n",
    "X = np.array([ValsToToken[x] for x in X_values])\n",
    "a = np.random.randint(0, len(tokenToVals), 10)\n",
    "\n",
    "print(X[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "block_size = 128\n",
    "batch_size = 64\n",
    "n_embd = 192 # be a multiple of n_head\n",
    "dropout = 0.2\n",
    "n_heads = 6 # be a divisible of n_embd\n",
    "vocab_size = len(tokenToVals) # ~= 1800\n",
    "n_layer = 4\n",
    "eval_iters = 100\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor([ValsToToken[x] for x in X_values], dtype=torch.long, device=device)\n",
    "n = int(0.9*len(X))\n",
    "X_train = X[:n]\n",
    "X_val = X[n:]\n",
    "\n",
    "# Single head self-attention\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input of size (batch, time-step, channels)\n",
    "        # output of size (batch, time-step, head size)\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,hs)\n",
    "        q = self.query(x) # (B,T,hs)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,hs)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
    "        return out\n",
    "# multi-head self-attention\n",
    "class MultiHead(nn.Module):\n",
    "    \"\"\" multi-head self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, n_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(n_heads)])\n",
    "        self.proj = nn.Linear(n_heads * head_size, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # input of size (batch, time-step, channels)\n",
    "        # output of size (batch, time-step, channels)\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "# feed-forward layer\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "    \n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, n_embd * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_embd * 4, n_embd),\n",
    "            nn.Dropout()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        return out\n",
    "# block\n",
    "class Block(nn.Module):\n",
    "    \"\"\" a transformer Block \"\"\"\n",
    "    \n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.MultiHeads = MultiHead(n_heads, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.MultiHeads(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "# transformer\n",
    "class Transformer(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_heads) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        # print('oouais', idx.shape)\n",
    "        B, T = idx.shape\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        # pos_emb = self.position_embedding_table(torch.arange(T)) # (T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:] # (B, T)\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "\n",
    "model = Transformer()\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = X_train if split == 'train' else X_val\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            # print(X.shape,Y.shape)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    # print(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1822517 parameters\n",
      "step 0: train loss 3.9886, val loss 3.9565\tlr : 0.05\t best val loss : 3.9565|at : 0\n",
      "step 500: train loss 3.1918, val loss 3.2683\tlr : 0.05\t best val loss : 3.2683|at : 500\n",
      "step 1000: train loss 3.2046, val loss 3.2988\tlr : 0.05\t best val loss : 3.2683|at : 500\n",
      "step 1500: train loss 3.2832, val loss 3.3457\tlr : 0.05\t best val loss : 3.2683|at : 500\n",
      "step 2000: train loss 3.2188, val loss 3.3371\tlr : 0.05\t best val loss : 3.2683|at : 500\n",
      "step 2500: train loss 3.1301, val loss 3.2068\tlr : 0.05\t best val loss : 3.2068|at : 2500\n",
      "step 3000: train loss 3.4134, val loss 3.4272\tlr : 0.05\t best val loss : 3.2068|at : 2500\n",
      "step 3500: train loss 3.1353, val loss 3.2167\tlr : 0.05\t best val loss : 3.2068|at : 2500\n",
      "step 4000: train loss 3.0661, val loss 3.1675\tlr : 0.05\t best val loss : 3.1675|at : 4000\n",
      "step 4500: train loss 3.0808, val loss 3.1832\tlr : 0.05\t best val loss : 3.1675|at : 4000\n",
      "step 5000: train loss 3.1439, val loss 3.3015\tlr : 0.05\t best val loss : 3.1675|at : 4000\n",
      "step 5500: train loss 3.3187, val loss 3.5460\tlr : 0.05\t best val loss : 3.1675|at : 4000\n",
      "step 6000: train loss 3.1208, val loss 3.2711\tlr : 0.05\t best val loss : 3.1675|at : 4000\n",
      "step 6500: train loss 3.1505, val loss 3.3029\tlr : 0.05\t best val loss : 3.1675|at : 4000\n",
      "step 7000: train loss 3.0799, val loss 3.1642\tlr : 0.05\t best val loss : 3.1642|at : 7000\n",
      "step 7500: train loss 3.1759, val loss 3.2253\tlr : 0.05\t best val loss : 3.1642|at : 7000\n",
      "step 8000: train loss 3.2483, val loss 3.2941\tlr : 0.05\t best val loss : 3.1642|at : 7000\n",
      "step 8500: train loss 3.4222, val loss 3.5670\tlr : 0.05\t best val loss : 3.1642|at : 7000\n",
      "step 9000: train loss 3.2283, val loss 3.2639\tlr : 0.05\t best val loss : 3.1642|at : 7000\n",
      "step 9500: train loss 3.2242, val loss 3.3241\tlr : 0.05\t best val loss : 3.1642|at : 7000\n",
      "step 10000: train loss 3.2033, val loss 3.3283\tlr : 0.05\t best val loss : 3.1642|at : 7000\n",
      "step 10500: train loss 3.3537, val loss 3.4027\tlr : 0.05\t best val loss : 3.1642|at : 7000\n",
      "step 11000: train loss 3.4557, val loss 3.5373\tlr : 0.05\t best val loss : 3.1642|at : 7000\n",
      "step 11500: train loss 3.3560, val loss 3.3725\tlr : 0.025\t best val loss : 3.1642|at : 7000\n",
      "step 12000: train loss 3.1581, val loss 3.2788\tlr : 0.025\t best val loss : 3.1642|at : 7000\n",
      "step 12500: train loss 3.1273, val loss 3.2404\tlr : 0.025\t best val loss : 3.1642|at : 7000\n",
      "step 13000: train loss 3.1939, val loss 3.3725\tlr : 0.025\t best val loss : 3.1642|at : 7000\n",
      "step 13500: train loss 3.0992, val loss 3.2188\tlr : 0.025\t best val loss : 3.1642|at : 7000\n",
      "step 14000: train loss 3.0239, val loss 3.1295\tlr : 0.025\t best val loss : 3.1295|at : 14000\n",
      "step 14500: train loss 3.0728, val loss 3.1423\tlr : 0.025\t best val loss : 3.1295|at : 14000\n",
      "step 15000: train loss 3.0338, val loss 3.1574\tlr : 0.025\t best val loss : 3.1295|at : 14000\n",
      "step 15500: train loss 3.0977, val loss 3.2155\tlr : 0.025\t best val loss : 3.1295|at : 14000\n",
      "step 16000: train loss 3.1607, val loss 3.2058\tlr : 0.025\t best val loss : 3.1295|at : 14000\n",
      "step 16500: train loss 3.0874, val loss 3.1596\tlr : 0.025\t best val loss : 3.1295|at : 14000\n",
      "step 17000: train loss 3.0352, val loss 3.0833\tlr : 0.025\t best val loss : 3.0833|at : 17000\n",
      "step 17500: train loss 3.0458, val loss 3.1041\tlr : 0.025\t best val loss : 3.0833|at : 17000\n",
      "step 18000: train loss 3.0308, val loss 3.1193\tlr : 0.025\t best val loss : 3.0833|at : 17000\n",
      "step 18500: train loss 3.0558, val loss 3.1755\tlr : 0.025\t best val loss : 3.0833|at : 17000\n",
      "step 19000: train loss 3.0678, val loss 3.2096\tlr : 0.025\t best val loss : 3.0833|at : 17000\n",
      "step 19500: train loss 2.9872, val loss 3.0594\tlr : 0.025\t best val loss : 3.0594|at : 19500\n",
      "step 20000: train loss 3.0888, val loss 3.1426\tlr : 0.025\t best val loss : 3.0594|at : 19500\n",
      "step 20500: train loss 3.1258, val loss 3.2519\tlr : 0.025\t best val loss : 3.0594|at : 19500\n",
      "step 21000: train loss 3.1307, val loss 3.1752\tlr : 0.025\t best val loss : 3.0594|at : 19500\n",
      "step 21500: train loss 3.0120, val loss 3.0896\tlr : 0.025\t best val loss : 3.0594|at : 19500\n",
      "step 22000: train loss 3.3546, val loss 3.5271\tlr : 0.025\t best val loss : 3.0594|at : 19500\n",
      "step 22500: train loss 3.1023, val loss 3.2603\tlr : 0.025\t best val loss : 3.0594|at : 19500\n",
      "step 23000: train loss 3.2508, val loss 3.4752\tlr : 0.025\t best val loss : 3.0594|at : 19500\n",
      "step 23500: train loss 3.2573, val loss 3.4201\tlr : 0.025\t best val loss : 3.0594|at : 19500\n",
      "step 24000: train loss 3.2432, val loss 3.3658\tlr : 0.0125\t best val loss : 3.0594|at : 19500\n",
      "step 24500: train loss 3.1071, val loss 3.2565\tlr : 0.0125\t best val loss : 3.0594|at : 19500\n",
      "step 25000: train loss 3.1431, val loss 3.2717\tlr : 0.0125\t best val loss : 3.0594|at : 19500\n",
      "step 25500: train loss 3.0690, val loss 3.2207\tlr : 0.0125\t best val loss : 3.0594|at : 19500\n",
      "step 26000: train loss 3.1150, val loss 3.2737\tlr : 0.0125\t best val loss : 3.0594|at : 19500\n",
      "step 26500: train loss 3.0406, val loss 3.1643\tlr : 0.0125\t best val loss : 3.0594|at : 19500\n",
      "step 27000: train loss 3.0163, val loss 3.1491\tlr : 0.0125\t best val loss : 3.0594|at : 19500\n",
      "step 27500: train loss 3.0185, val loss 3.1541\tlr : 0.0125\t best val loss : 3.0594|at : 19500\n",
      "step 28000: train loss 2.9353, val loss 3.0968\tlr : 0.0125\t best val loss : 3.0594|at : 19500\n",
      "step 28500: train loss 2.9846, val loss 3.1414\tlr : 0.00625\t best val loss : 3.0594|at : 19500\n",
      "step 29000: train loss 2.9105, val loss 3.0467\tlr : 0.00625\t best val loss : 3.0467|at : 29000\n",
      "step 29500: train loss 2.8611, val loss 2.9848\tlr : 0.00625\t best val loss : 2.9848|at : 29500\n",
      "step 30000: train loss 2.8146, val loss 2.9416\tlr : 0.00625\t best val loss : 2.9416|at : 30000\n",
      "step 30500: train loss 2.8231, val loss 2.9115\tlr : 0.00625\t best val loss : 2.9115|at : 30500\n",
      "step 31000: train loss 2.8413, val loss 2.9262\tlr : 0.00625\t best val loss : 2.9115|at : 30500\n",
      "step 31500: train loss 2.7983, val loss 2.8781\tlr : 0.00625\t best val loss : 2.8781|at : 31500\n",
      "step 32000: train loss 2.7996, val loss 2.8971\tlr : 0.00625\t best val loss : 2.8781|at : 31500\n",
      "step 32500: train loss 2.8133, val loss 2.9224\tlr : 0.00625\t best val loss : 2.8781|at : 31500\n",
      "step 33000: train loss 2.8249, val loss 2.9361\tlr : 0.00625\t best val loss : 2.8781|at : 31500\n",
      "step 33500: train loss 2.8232, val loss 2.9171\tlr : 0.00625\t best val loss : 2.8781|at : 31500\n",
      "step 34000: train loss 2.8841, val loss 3.0480\tlr : 0.00625\t best val loss : 2.8781|at : 31500\n",
      "step 34500: train loss 2.8602, val loss 2.9847\tlr : 0.00625\t best val loss : 2.8781|at : 31500\n",
      "step 35000: train loss 2.8425, val loss 2.9582\tlr : 0.00625\t best val loss : 2.8781|at : 31500\n",
      "step 35500: train loss 3.0277, val loss 3.1862\tlr : 0.00625\t best val loss : 2.8781|at : 31500\n",
      "step 36000: train loss 3.0723, val loss 3.2518\tlr : 0.003125\t best val loss : 2.8781|at : 31500\n",
      "step 36500: train loss 3.0217, val loss 3.2119\tlr : 0.003125\t best val loss : 2.8781|at : 31500\n",
      "step 37000: train loss 2.9472, val loss 3.1347\tlr : 0.003125\t best val loss : 2.8781|at : 31500\n",
      "step 37500: train loss 2.9637, val loss 3.1321\tlr : 0.003125\t best val loss : 2.8781|at : 31500\n",
      "step 38000: train loss 2.8843, val loss 3.0741\tlr : 0.003125\t best val loss : 2.8781|at : 31500\n",
      "step 38500: train loss 2.8643, val loss 3.0273\tlr : 0.003125\t best val loss : 2.8781|at : 31500\n",
      "step 39000: train loss 2.8474, val loss 3.0234\tlr : 0.003125\t best val loss : 2.8781|at : 31500\n",
      "step 39500: train loss 2.8496, val loss 3.0127\tlr : 0.003125\t best val loss : 2.8781|at : 31500\n",
      "step 40000: train loss 2.8432, val loss 2.9919\tlr : 0.003125\t best val loss : 2.8781|at : 31500\n",
      "step 40500: train loss 2.8099, val loss 2.9526\tlr : 0.0015625\t best val loss : 2.8781|at : 31500\n",
      "step 41000: train loss 2.7723, val loss 2.9231\tlr : 0.0015625\t best val loss : 2.8781|at : 31500\n",
      "step 41500: train loss 2.7363, val loss 2.8706\tlr : 0.0015625\t best val loss : 2.8706|at : 41500\n",
      "step 42000: train loss 2.7275, val loss 2.8697\tlr : 0.0015625\t best val loss : 2.8697|at : 42000\n",
      "step 42500: train loss 2.7261, val loss 2.8663\tlr : 0.0015625\t best val loss : 2.8663|at : 42500\n",
      "step 43000: train loss 2.7158, val loss 2.8504\tlr : 0.0015625\t best val loss : 2.8504|at : 43000\n",
      "step 43500: train loss 2.7003, val loss 2.8129\tlr : 0.0015625\t best val loss : 2.8129|at : 43500\n",
      "step 44000: train loss 2.7033, val loss 2.8167\tlr : 0.0015625\t best val loss : 2.8129|at : 43500\n",
      "step 44500: train loss 2.6935, val loss 2.8157\tlr : 0.0015625\t best val loss : 2.8129|at : 43500\n",
      "step 45000: train loss 2.6812, val loss 2.8030\tlr : 0.0015625\t best val loss : 2.8030|at : 45000\n",
      "step 45500: train loss 2.6757, val loss 2.7960\tlr : 0.0015625\t best val loss : 2.7960|at : 45500\n",
      "step 46000: train loss 2.6861, val loss 2.8140\tlr : 0.0015625\t best val loss : 2.7960|at : 45500\n",
      "step 46500: train loss 2.6610, val loss 2.7881\tlr : 0.0015625\t best val loss : 2.7881|at : 46500\n",
      "step 47000: train loss 2.6624, val loss 2.7893\tlr : 0.0015625\t best val loss : 2.7881|at : 46500\n",
      "step 47500: train loss 2.6493, val loss 2.7821\tlr : 0.0015625\t best val loss : 2.7821|at : 47500\n",
      "step 48000: train loss 2.6673, val loss 2.7907\tlr : 0.0015625\t best val loss : 2.7821|at : 47500\n",
      "step 48500: train loss 2.6733, val loss 2.7904\tlr : 0.0015625\t best val loss : 2.7821|at : 47500\n",
      "step 49000: train loss 2.6727, val loss 2.7998\tlr : 0.0015625\t best val loss : 2.7821|at : 47500\n",
      "step 49500: train loss 2.6701, val loss 2.8228\tlr : 0.0015625\t best val loss : 2.7821|at : 47500\n",
      "step 50000: train loss 2.6588, val loss 2.7973\tlr : 0.0015625\t best val loss : 2.7821|at : 47500\n",
      "step 50500: train loss 2.6327, val loss 2.7849\tlr : 0.0015625\t best val loss : 2.7821|at : 47500\n",
      "step 51000: train loss 2.6309, val loss 2.7693\tlr : 0.0015625\t best val loss : 2.7693|at : 51000\n",
      "step 51500: train loss 2.6364, val loss 2.7739\tlr : 0.0015625\t best val loss : 2.7693|at : 51000\n",
      "step 52000: train loss 2.6008, val loss 2.7473\tlr : 0.0015625\t best val loss : 2.7473|at : 52000\n",
      "step 52500: train loss 2.5893, val loss 2.7311\tlr : 0.0015625\t best val loss : 2.7311|at : 52500\n",
      "step 53000: train loss 2.5742, val loss 2.7172\tlr : 0.0015625\t best val loss : 2.7172|at : 53000\n",
      "step 53500: train loss 2.5847, val loss 2.7243\tlr : 0.0015625\t best val loss : 2.7172|at : 53000\n",
      "step 54000: train loss 2.5614, val loss 2.7198\tlr : 0.0015625\t best val loss : 2.7172|at : 53000\n",
      "step 54500: train loss 2.5699, val loss 2.7193\tlr : 0.0015625\t best val loss : 2.7172|at : 53000\n",
      "step 55000: train loss 2.5420, val loss 2.7058\tlr : 0.0015625\t best val loss : 2.7058|at : 55000\n",
      "step 55500: train loss 2.5385, val loss 2.6908\tlr : 0.0015625\t best val loss : 2.6908|at : 55500\n",
      "step 56000: train loss 2.5514, val loss 2.6850\tlr : 0.0015625\t best val loss : 2.6850|at : 56000\n",
      "step 56500: train loss 2.5377, val loss 2.6957\tlr : 0.0015625\t best val loss : 2.6850|at : 56000\n",
      "step 57000: train loss 2.5464, val loss 2.7123\tlr : 0.0015625\t best val loss : 2.6850|at : 56000\n",
      "step 57500: train loss 2.5451, val loss 2.6887\tlr : 0.0015625\t best val loss : 2.6850|at : 56000\n",
      "step 58000: train loss 2.5245, val loss 2.6792\tlr : 0.0015625\t best val loss : 2.6792|at : 58000\n",
      "step 58500: train loss 2.5268, val loss 2.6939\tlr : 0.0015625\t best val loss : 2.6792|at : 58000\n",
      "step 59000: train loss 2.5371, val loss 2.6973\tlr : 0.0015625\t best val loss : 2.6792|at : 58000\n",
      "step 59500: train loss 2.5193, val loss 2.6899\tlr : 0.0015625\t best val loss : 2.6792|at : 58000\n",
      "step 60000: train loss 2.4982, val loss 2.6825\tlr : 0.0015625\t best val loss : 2.6792|at : 58000\n",
      "step 60500: train loss 2.5214, val loss 2.7094\tlr : 0.0015625\t best val loss : 2.6792|at : 58000\n",
      "step 61000: train loss 2.5204, val loss 2.6914\tlr : 0.0015625\t best val loss : 2.6792|at : 58000\n",
      "step 61500: train loss 2.4975, val loss 2.6771\tlr : 0.0015625\t best val loss : 2.6771|at : 61500\n",
      "step 62000: train loss 2.4867, val loss 2.6599\tlr : 0.0015625\t best val loss : 2.6599|at : 62000\n",
      "step 62500: train loss 2.4873, val loss 2.6623\tlr : 0.0015625\t best val loss : 2.6599|at : 62000\n",
      "step 63000: train loss 2.4694, val loss 2.6747\tlr : 0.0015625\t best val loss : 2.6599|at : 62000\n",
      "step 63500: train loss 2.4769, val loss 2.6670\tlr : 0.0015625\t best val loss : 2.6599|at : 62000\n",
      "step 64000: train loss 2.4714, val loss 2.6478\tlr : 0.0015625\t best val loss : 2.6478|at : 64000\n",
      "step 64500: train loss 2.4542, val loss 2.6387\tlr : 0.0015625\t best val loss : 2.6387|at : 64500\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: unknown error\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 37\u001b[0m\n\u001b[0;32m     35\u001b[0m     logits, loss \u001b[38;5;241m=\u001b[39m model(xb, yb)\n\u001b[0;32m     36\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 37\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;66;03m# scheduler.step()\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \n\u001b[0;32m     41\u001b[0m \n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# save the model\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lorra\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lorra\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: unknown error\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "max_iters = 150000\n",
    "eval_interval = 500\n",
    "lossi = []\n",
    "learning_rate = 0.05\n",
    "best_val_loss = float('inf')\n",
    "stepBestVal = 0\n",
    "print(sum(p.numel() for p in model.parameters()), 'parameters')\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "# scheduler = lr_scheduler.StepLR(optimizer, step_size=2000, gamma=0.3)\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10, verbose=True)\n",
    "# scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=max_iters, eta_min=0, last_epoch=-1)\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        lossi.append(losses)\n",
    "        # update the learning rate if the validation loss has stopped improving\n",
    "        if losses['val'] < best_val_loss:\n",
    "            stepBestVal = iter\n",
    "            best_val_loss = losses['val']\n",
    "            scheduler.step(losses['val'])\n",
    "        else:\n",
    "            scheduler.step(losses['val'])\n",
    "        \n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\\tlr : {optimizer.param_groups[0]['lr']}\\t best val loss : {best_val_loss:.4f}|at : {stepBestVal}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # scheduler.step()\n",
    "\n",
    "\n",
    "# save the model\n",
    "torch.save(model.state_dict(), 'model1.pth')\n",
    "\n",
    "print(lossi)\n",
    "plt.plot([l['train'] for l in lossi], label='train')\n",
    "plt.plot([l['val'] for l in lossi], label='val')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# converting it back to music\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[48, 51, 48, 53, 63, 50, 56, 60, 56, 54, 56, 61, 63, 55, 56, 66, 66, 56, 48, 55, 56, 49, 57, 62, 58, 55, 56, 83, 53, 61, 54, 48, 56, 63, 61, 73, 73, 73, 68, 69, 59, 51, 56, 59, 70, 75, 71, 66, 59, 57, 55, 59, 59, 64, 62, 55, 59, 59, 62, 56, 68, 68, 71, 59, 69, 68, 68, 60, 64, 56, 60, 66, 59, 93, 61, 70, 68, 56, 69, 66, 56, 56, 71, 75, 73, 80, 72, 73, 65, 66, 66, 62, 71, 69, 69, 64, 56, 64, 63, 60, 60]\n",
      "[0, 5, 5, 13, 10, 14, 10, 5, 12, 10, 12, 5, 10, 3, 2, 10, 11, 9, 10, 7, 1, 22, 19, 15, 10, 10, 14, 10, 5, 15, 10, 17, 6, 10, 12, 14, 12, 3, 9, 15, 3, 14, 8, 14, 10, 15, 17, 10, 15, 3, 3, 5, 17, 14, 10, 9, 10, 10, 15, 27, 18, 3, 3, 11, 13, 31, 12, 11, 11, 12, 14, 10, 13, 12, 15, 10, 0, 10, 16, 11, 13, 10, 12, 25, 19, 17, 17, 17, 5, 12, 6, 10, 10, 12, 22, 13, 17, 27, 25, 24, 17]\n",
      "[48, 53, 53, 61, 58, 62, 58, 53, 60, 58, 60, 53, 58, 51, 50, 58, 59, 57, 58, 55, 49, 70, 67, 63, 58, 58, 62, 58, 53, 63, 58, 65, 54, 58, 60, 62, 60, 51, 57, 63, 51, 62, 56, 62, 58, 63, 65, 58, 63, 51, 51, 53, 65, 62, 58, 57, 58, 58, 63, 75, 66, 51, 51, 59, 61, 79, 60, 59, 59, 60, 62, 58, 61, 60, 63, 58, 48, 58, 64, 59, 61, 58, 60, 73, 67, 65, 65, 65, 53, 60, 54, 58, 58, 60, 70, 61, 65, 75, 73, 72, 65]\n"
     ]
    }
   ],
   "source": [
    "decode = lambda x: [tokenToVals[i] for i in x]\n",
    "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "print(decode(model.generate(context, max_new_tokens=100)[0].tolist()))\n",
    "\n",
    "def tokensToMidi(tokens):\n",
    "    vals = decode(tokens)\n",
    "    print(vals)\n",
    "    notes = []\n",
    "    for t in vals:\n",
    "        # print(type(t))\n",
    "        if(np.equal(t, 0)): continue\n",
    "        notes.append(t)\n",
    "    createMidiFileWithAllMessages(notes, filename='test.mid')\n",
    "\n",
    "\n",
    "gen1 = model.generate(context, max_new_tokens=100)[0].tolist()\n",
    "\n",
    "\n",
    "print(gen1)\n",
    "tokensToMidi(gen1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
