{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mido\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mido\n",
    "\n",
    "def createMidiFileWithAllMessages(notes, duration=200, wait_time = 0, filename='test.mid'):\n",
    "    mid = mido.MidiFile()\n",
    "    track = mido.MidiTrack()\n",
    "    mid.tracks.append(track)\n",
    "\n",
    "    for note in notes:\n",
    "        track.append(mido.Message('note_on', note=note, velocity=64, time=wait_time))\n",
    "        track.append(mido.Message('note_off', note=note, velocity=0, time=duration))\n",
    "\n",
    "    mid.save(filename.removesuffix('.mid') + '.mid')\n",
    "\n",
    "\n",
    "notes = range(50, 80) \n",
    "\n",
    "createMidiFileWithAllMessages(notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "catch all the data\n",
      "['data\\\\albeniz\\\\alb_esp1.mid', 'data\\\\albeniz\\\\alb_esp2.mid', 'data\\\\albeniz\\\\alb_esp3.mid', 'data\\\\albeniz\\\\alb_esp4.mid', 'data\\\\albeniz\\\\alb_esp5.mid', 'data\\\\albeniz\\\\alb_esp6.mid', 'data\\\\albeniz\\\\alb_se1.mid', 'data\\\\albeniz\\\\alb_se2.mid', 'data\\\\albeniz\\\\alb_se3.mid', 'data\\\\albeniz\\\\alb_se4.mid']\n",
      "295\n"
     ]
    }
   ],
   "source": [
    "print('catch all the data')\n",
    "\n",
    "def get_midis():\n",
    "    ms = []\n",
    "    directory = \"data\"\n",
    "    for foldername in os.listdir(directory):\n",
    "        directory = os.path.join(\"data\", foldername)\n",
    "        for filename in os.listdir(directory):\n",
    "            if('V2'in filename): continue\n",
    "            if os.path.isfile(os.path.join(directory, filename)):\n",
    "                ms.append(os.path.join(directory, filename))\n",
    "                # print(filename)\n",
    "    return ms\n",
    "\n",
    "midis = get_midis()\n",
    "print(midis[:10])\n",
    "# works only with files with just one track\n",
    "def get_all_notes_one_file(midi_file):\n",
    "    file = mido.MidiFile(midi_file)\n",
    "    track = file.tracks[1]\n",
    "    messages = []\n",
    "    for message in track:\n",
    "        if(message.type == 'note_on'):\n",
    "            messages.append(message)\n",
    "    \n",
    "    notes = [0] + [m.note for m in messages] + [1]\n",
    "    # print(notes)\n",
    "    return notes\n",
    "\n",
    "all_notes_all_files = [get_all_notes_one_file(m) for m in midis]\n",
    "\n",
    "print(len(all_notes_all_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(763088,)\n",
      "notes extreme to remove :  [ 24  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  42 104\n",
      " 105 106 107]\n",
      "[ 0 41 41 48 48 46 46 48 48 46]\n"
     ]
    }
   ],
   "source": [
    "X_values = np.concatenate(all_notes_all_files)\n",
    "print(X_values.shape)\n",
    "\n",
    "# notes\n",
    "\n",
    "n = np.unique(X_values, return_counts=True)\n",
    "notes_extreme = n[0][n[1] < 100]\n",
    "print('notes extreme to remove : ', notes_extreme)\n",
    "X_values = X_values[~np.isin(X_values, notes_extreme)]\n",
    "\n",
    "nunique = np.unique(X_values, return_counts=True)\n",
    "\n",
    "# Tokenize the data\n",
    "\n",
    "tokenToVals = nunique[0]\n",
    "\n",
    "ValsToToken = {v:i for i, v in enumerate(tokenToVals)}\n",
    "vocab_size = len(tokenToVals)\n",
    "\n",
    "X = np.array([ValsToToken[x] for x in X_values])\n",
    "a = np.random.randint(0, len(tokenToVals), 10)\n",
    "\n",
    "print(X[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "block_size = 128\n",
    "batch_size = 64\n",
    "n_embd = 192 # be a multiple of n_head\n",
    "dropout = 0.2\n",
    "n_heads = 6 # be a divisible of n_embd\n",
    "vocab_size = len(tokenToVals) # ~= 1800\n",
    "n_layer = 4\n",
    "eval_iters = 100\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor([ValsToToken[x] for x in X_values], dtype=torch.long, device=device)\n",
    "n = int(0.9*len(X))\n",
    "X_train = X[:n]\n",
    "X_val = X[n:]\n",
    "\n",
    "# Single head self-attention\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input of size (batch, time-step, channels)\n",
    "        # output of size (batch, time-step, head size)\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,hs)\n",
    "        q = self.query(x) # (B,T,hs)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,hs)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
    "        return out\n",
    "# multi-head self-attention\n",
    "class MultiHead(nn.Module):\n",
    "    \"\"\" multi-head self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, n_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(n_heads)])\n",
    "        self.proj = nn.Linear(n_heads * head_size, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # input of size (batch, time-step, channels)\n",
    "        # output of size (batch, time-step, channels)\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "# feed-forward layer\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "    \n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, n_embd * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_embd * 4, n_embd),\n",
    "            nn.Dropout()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        return out\n",
    "# block\n",
    "class Block(nn.Module):\n",
    "    \"\"\" a transformer Block \"\"\"\n",
    "    \n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.MultiHeads = MultiHead(n_heads, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.MultiHeads(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "# transformer\n",
    "class Transformer(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_heads) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        # print('oouais', idx.shape)\n",
    "        B, T = idx.shape\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        # pos_emb = self.position_embedding_table(torch.arange(T)) # (T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:] # (B, T)\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "\n",
    "model = Transformer()\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = X_train if split == 'train' else X_val\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            # print(X.shape,Y.shape)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    # print(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1826752 parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lorra\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\lorra\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.2409, val loss 4.2376\tlr : 0.05\t best val loss : 4.2376|at : 0\n",
      "step 400: train loss 3.5178, val loss 3.5034\tlr : 0.05\t best val loss : 3.5034|at : 400\n",
      "step 800: train loss 3.2723, val loss 3.2936\tlr : 0.05\t best val loss : 3.2936|at : 800\n",
      "step 1200: train loss 3.3041, val loss 3.2896\tlr : 0.05\t best val loss : 3.2896|at : 1200\n",
      "step 1600: train loss 3.2196, val loss 3.2340\tlr : 0.05\t best val loss : 3.2340|at : 1600\n",
      "step 2000: train loss 3.2699, val loss 3.2693\tlr : 0.05\t best val loss : 3.2340|at : 1600\n",
      "step 2400: train loss 3.1730, val loss 3.2146\tlr : 0.05\t best val loss : 3.2146|at : 2400\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 38\u001b[0m\n\u001b[0;32m     36\u001b[0m     logits, loss \u001b[38;5;241m=\u001b[39m model(xb, yb)\n\u001b[0;32m     37\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 38\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;66;03m# scheduler.step()\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \n\u001b[0;32m     42\u001b[0m \n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# save the model\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lorra\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lorra\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "max_iters = 40000\n",
    "eval_interval = 400\n",
    "lossi = []\n",
    "learning_rate = 0.05\n",
    "best_val_loss = float('inf')\n",
    "stepBestVal = 0\n",
    "print(sum(p.numel() for p in model.parameters()), 'parameters')\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "# scheduler = lr_scheduler.StepLR(optimizer, step_size=2000, gamma=0.3)\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=7, verbose=True)\n",
    "# lr_scheduler.CosineAnnealingLR(optimizer, T_max=1000, eta_min=0.0001, last_epoch=-1)\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        lossi.append(losses)\n",
    "\n",
    "        # update the learning rate if the validation loss has stopped improving\n",
    "        if losses['val'] < best_val_loss:\n",
    "            stepBestVal = iter\n",
    "            best_val_loss = losses['val']\n",
    "            scheduler.step(losses['val'])\n",
    "        else:\n",
    "            scheduler.step(losses['val'])\n",
    "            \n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\\tlr : {optimizer.param_groups[0]['lr']}\\t best val loss : {best_val_loss:.4f}|at : {stepBestVal}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # scheduler.step()\n",
    "\n",
    "\n",
    "# save the model\n",
    "torch.save(model.state_dict(), 'model1.pth')\n",
    "\n",
    "print(lossi)\n",
    "plt.plot([l['train'] for l in lossi], label='train')\n",
    "plt.plot([l['val'] for l in lossi], label='val')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# converting it back to music\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 62, 65, 70, 67, 72, 58, 75, 76, 77, 69, 70, 70, 75, 75, 74, 75, 75, 71, 74, 77, 79, 75, 73, 72, 65, 75, 77, 74, 69, 72, 76, 72, 75, 83, 69, 71, 77, 85, 88, 87, 87, 87, 79, 82, 70, 77, 89, 77, 65, 88, 75, 80, 70, 63, 82, 80, 80, 82, 72, 69, 70, 74, 75, 79, 75, 79, 75, 83, 72, 68, 60, 72, 74, 72, 72, 68, 75, 77, 82, 84, 83, 72, 80, 73, 75, 70, 73, 70, 68, 68, 69, 70, 79, 82, 82, 72, 70, 74, 73, 72]\n",
      "[0, 30, 33, 34, 30, 28, 31, 35, 32, 35, 25, 32, 30, 34, 30, 28, 32, 26, 31, 27, 26, 24, 30, 27, 30, 40, 32, 32, 32, 29, 31, 34, 37, 30, 28, 28, 33, 37, 34, 37, 42, 44, 39, 23, 23, 32, 25, 39, 35, 34, 34, 35, 23, 14, 40, 47, 38, 33, 32, 32, 28, 28, 39, 32, 32, 36, 32, 32, 23, 23, 20, 32, 29, 22, 39, 34, 34, 38, 46, 30, 34, 36, 31, 47, 22, 18, 23, 25, 32, 34, 37, 37, 44, 33, 35, 38, 34, 37, 37, 39, 39]\n",
      "[0, 70, 73, 74, 70, 68, 71, 75, 72, 75, 65, 72, 70, 74, 70, 68, 72, 66, 71, 67, 66, 64, 70, 67, 70, 80, 72, 72, 72, 69, 71, 74, 77, 70, 68, 68, 73, 77, 74, 77, 82, 84, 79, 63, 63, 72, 65, 79, 75, 74, 74, 75, 63, 54, 80, 87, 78, 73, 72, 72, 68, 68, 79, 72, 72, 76, 72, 72, 63, 63, 60, 72, 69, 62, 79, 74, 74, 78, 86, 70, 74, 76, 71, 87, 62, 58, 63, 65, 72, 74, 77, 77, 84, 73, 75, 78, 74, 77, 77, 79, 79]\n"
     ]
    }
   ],
   "source": [
    "decode = lambda x: [tokenToVals[i] for i in x]\n",
    "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "print(decode(model.generate(context, max_new_tokens=100)[0].tolist()))\n",
    "\n",
    "def tokensToMidi(tokens):\n",
    "    vals = decode(tokens)\n",
    "    print(vals)\n",
    "    notes = []\n",
    "    for t in vals:\n",
    "        # print(type(t))\n",
    "        if(np.equal(t, 0)): continue\n",
    "        notes.append(t)\n",
    "    createMidiFileWithAllMessages(notes, filename='test.mid')\n",
    "\n",
    "\n",
    "gen1 = model.generate(context, max_new_tokens=100)[0].tolist()\n",
    "\n",
    "\n",
    "print(gen1)\n",
    "tokensToMidi(gen1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
